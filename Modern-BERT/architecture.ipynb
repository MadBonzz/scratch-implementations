{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b42f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6fec16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33e88859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, base, max_len, head_dim):\n",
    "        super(RoPE, self).__init__()\n",
    "        assert head_dim % 2 == 0\n",
    "        self.thetas = 1 / (torch.pow(base, torch.arange(0, head_dim, 2) / head_dim)).float()\n",
    "        self.seqs = torch.arange(max_len)\n",
    "        self.matrix = torch.outer(self.seqs, self.thetas)\n",
    "        self.complex = torch.polar(torch.ones_like(self.matrix), self.matrix).unsqueeze(0).unsqueeze(2)\n",
    "        self.register_buffer('freqs_complex', self.complex)\n",
    "    def forward(self, x):\n",
    "        B, T, n_heads, head_dim = x.shape\n",
    "        complex_x = torch.view_as_complex(x.float().reshape(B, T, n_heads, head_dim // 2, 2))\n",
    "        rotated_x = complex_x * self.freqs_complex[:, :T, :, :]\n",
    "        rotated_x = torch.view_as_real(rotated_x).reshape(*x.shape).type_as(x)\n",
    "        return rotated_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83860a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, n_embed, eps=1e-5):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.gammas = nn.Parameter(torch.ones(n_embed))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        rms = torch.pow(torch.mean(torch.pow(x, 2), dim=2), 1/2).unsqueeze(-1)\n",
    "        x = x / (rms + self.eps)\n",
    "        return x * self.gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37ddf64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeGlu(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeGlu, self).__init__()\n",
    "        self.up_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.down_proj = nn.Linear(out_dim // 2, in_dim)\n",
    "        nn.init.xavier_uniform_(self.up_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.down_proj.weight)\n",
    "    def forward(self, x):\n",
    "        x, gate = self.up_proj(x).chunk(2, dim=-1)\n",
    "        return self.down_proj(x * F.gelu(gate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77316f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAttention(nn.Module):\n",
    "    def __init__(self, embed_dim : int, n_heads : int, dropout : float, rope):\n",
    "        super(GlobalAttention, self).__init__()\n",
    "        self.rope = rope\n",
    "        self.head_size = embed_dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.up_proj = nn.Linear(embed_dim, self.head_size * n_heads * 3)\n",
    "        self.down_proj = nn.Linear(self.head_size * n_heads, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        nn.init.xavier_uniform_(self.up_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.down_proj.weight)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.up_proj(x).split(self.n_heads * self.head_size, dim=-1)\n",
    "        q = self.rope(q.view(B, T, self.n_heads, self.head_size))\n",
    "        k = self.rope(k.view(B, T, self.n_heads, self.head_size))\n",
    "        v = v.view(B, T, self.n_heads, self.head_size)\n",
    "        attn = F.scaled_dot_product_attention(q, k, v)\n",
    "        attn = attn.reshape(B, T, self.n_heads * self.head_size)\n",
    "        return self.dropout(self.down_proj(attn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee0e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumsum(lst):\n",
    "    cum_sum = [0]\n",
    "    total = 0\n",
    "    for num in lst:\n",
    "        total += num\n",
    "        cum_sum.append(total)\n",
    "    return cum_sum\n",
    "\n",
    "def unpadding(inputs, attention_masks, device):\n",
    "    assert inputs.dim() == 2\n",
    "    inputs = inputs.flatten()\n",
    "    indices = attention_masks.bool()\n",
    "    cu_lens = [mask.sum().item() for mask in indices]\n",
    "    cu_lens = cumsum(cu_lens)\n",
    "    indices = indices.flatten()\n",
    "    return inputs[indices].to(device), cu_lens\n",
    "    \n",
    "def padding(inputs, attention_mask, batch_size, seq_len, device):\n",
    "    assert inputs.dim() == 1\n",
    "    outputs = torch.zeros(batch_size * seq_len, dtype=inputs.dtype, device=inputs.device)\n",
    "    outputs[attention_mask.bool().flatten()] = inputs\n",
    "    outputs = outputs.reshape(batch_size, seq_len)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac93039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(high=100, size=(4, 32)).to(device)\n",
    "a = torch.randint(high=2, size=(4, 32)).to(device)\n",
    "out, lens = unpadding(x, a, device)\n",
    "padded = padding(out, a, 4, 32, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0787c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1, q2, q3 = torch.rand(6, 32), torch.rand(8, 32), torch.rand(5, 32)\n",
    "k1, k2, k3 = torch.rand(6, 32), torch.rand(8, 32), torch.rand(5, 32)\n",
    "v1, v2, v3 = torch.rand(6, 32), torch.rand(8, 32), torch.rand(5, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6b41ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.nested.nested_tensor([q1, q2, q3], layout=torch.jagged).to(device)\n",
    "k = torch.nested.nested_tensor([k1, k2, k3], layout=torch.jagged).to(device)\n",
    "v = torch.nested.nested_tensor([v1, v2, v3], layout=torch.jagged).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d1d9d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0505 03:36:00.548000 29704 site-packages\\torch\\nested\\_internal\\sdpa.py:294] Memory efficient kernel not used because:\n",
      "W0505 03:36:00.550000 29704 site-packages\\torch\\nested\\_internal\\sdpa.py:122] Fused kernels do not support ragged num_head_dims, query has a ragged num_heads.\n",
      "W0505 03:36:00.552000 29704 site-packages\\torch\\nested\\_internal\\sdpa.py:297] Flash attention kernel not used because:\n",
      "W0505 03:36:00.553000 29704 site-packages\\torch\\nested\\_internal\\sdpa.py:122] Fused kernels do not support ragged num_head_dims, query has a ragged num_heads.\n",
      "W0505 03:36:00.554000 29704 site-packages\\torch\\nested\\_internal\\sdpa.py:300] Math attention kernel not used because:\n",
      "W0505 03:36:00.558000 29704 site-packages\\torch\\nested\\_internal\\sdpa.py:252] If inputs are nested tensors they must be contiguous after transposing.\n",
      "C:\\Users\\shour\\AppData\\Local\\Temp\\ipykernel_29704\\3563031360.py:4: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen/native/transformers/sdp_utils_cpp.h:551.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v)\n",
      "C:\\Users\\shour\\AppData\\Local\\Temp\\ipykernel_29704\\3563031360.py:4: UserWarning: Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:598.)\n",
      "  out = F.scaled_dot_product_attention(q, k, v)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No viable backend for scaled_dot_product_attention was found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mattention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SDPBackend, sdpa_kernel\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     out = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shour\\anaconda3\\envs\\general\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py:349\u001b[39m, in \u001b[36mNestedTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m maybe_enable_thunkify():\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjagged_torch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[32m    351\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shour\\anaconda3\\envs\\general\\Lib\\site-packages\\torch\\nested\\_internal\\ops.py:374\u001b[39m, in \u001b[36mjagged_torch_function\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjagged_torch_function\u001b[39m(func, *args, **kwargs):\n\u001b[32m    371\u001b[39m     \u001b[38;5;66;03m# SDPA has special kernels that handle nested tensors.\u001b[39;00m\n\u001b[32m    372\u001b[39m     \u001b[38;5;66;03m# Dispatch to the correct implementation here\u001b[39;00m\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m torch._C._nn.scaled_dot_product_attention:\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjagged_scaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func.\u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33mapply_\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    377\u001b[39m         func(args[\u001b[32m0\u001b[39m]._values, *args[\u001b[32m1\u001b[39m:], **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shour\\anaconda3\\envs\\general\\Lib\\site-packages\\torch\\nested\\_internal\\sdpa.py:860\u001b[39m, in \u001b[36mjagged_scaled_dot_product_attention\u001b[39m\u001b[34m(query, key, value, attn_mask, dropout_p, is_causal, scale, enable_gqa)\u001b[39m\n\u001b[32m    858\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_out\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m860\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    861\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo viable backend for scaled_dot_product_attention was found.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    862\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: No viable backend for scaled_dot_product_attention was found."
     ]
    }
   ],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "    out = F.scaled_dot_product_attention(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebf5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
